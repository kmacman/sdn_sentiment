{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44145d09",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18dcae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bf98daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the NLP sentiment tokenizers/models, and can be changed to test alternative models\n",
    "tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81aaf4",
   "metadata": {},
   "source": [
    "# Variables to Define\n",
    "\n",
    "These will depend on which board you want to scrape, how much you want to scrape, the title you want for your output CSVs, etc.\n",
    "Examples are filled in.\n",
    "\n",
    "board_url = the URL of the board you would like to scrape\n",
    "pages = how many pages of the board you'd like to scrape (check the website for total number of pages if you'd like to do a full scrape)\n",
    "project_title = will define the title for the CSVs outputted by the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "217dd0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_url = 'https://forums.studentdoctor.net/forums/family-medicine.37/'\n",
    "pages = 100\n",
    "project_title = 'SDNFamMed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781ca5ff",
   "metadata": {},
   "source": [
    "# Script\n",
    "Running this will output 6 CSVs:\n",
    "[title]_raw_data.csv: post timestamp + content\n",
    "[title]_data_sentiment.csv: post timestamp + content + sentiment analysis\n",
    "[title]_daily_average.csv: date + daily sentiment average\n",
    "[title]_weekly_average.csv: date + weekly sentiment average\n",
    "[title]_monthly_average.csv: month + monthly sentiment average\n",
    "[title]_yearly_average.csv: year + annual sentiment average\n",
    "\n",
    "These .csv's may have some posts with outlier datestamps due to stickied posts at the top of the first page, so they will need to be cleaned unless you are running a full scrape of the board from inception to present day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d56f6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://forums.studentdoctor.net/'\n",
    "forum_url_template = f\"{board_url}page-{{}}\" \n",
    "full_post_urls = []\n",
    "\n",
    "for page_number in range(1, pages):\n",
    "    board_url = forum_url_template.format(page_number)\n",
    "    board_html = requests.get(board_url)\n",
    "    board_soup = BeautifulSoup(board_html.text, 'html.parser')\n",
    "    posts = board_soup.find_all('div', attrs={'class':'structItem-title'})\n",
    "\n",
    "    for url in posts:\n",
    "        link = url.find('a')['href']\n",
    "        full_link = base_url + link\n",
    "        full_post_urls.append(full_link)\n",
    "        \n",
    "def extract_forum_data(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    result_date = [time['datetime'] for time in soup.find_all('time')]\n",
    "    result_text = [div.get_text(separator=' ', strip=True) for div in soup.find_all('div', attrs={'class':'bbWrapper'})]\n",
    "    return list(zip(result_date, result_text))\n",
    "\n",
    "all_data = [ ]\n",
    "for url in full_post_urls:\n",
    "  all_data.extend(extract_forum_data(url))\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=['Date','Content'])\n",
    "df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "df.to_csv(project_title+'_raw_data.csv', index=True)\n",
    "\n",
    "def sentiment_score(review):\n",
    "    tokens = tokenizer.encode(review, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits))/2\n",
    "\n",
    "df['Sentiment'] = df['Content'].apply(lambda x: sentiment_score(x[:512]))\n",
    "\n",
    "df.to_csv(project_title+'_data_sentiment.csv', index=True)\n",
    "\n",
    "def resample_data(df, freq):\n",
    "    return df['Sentiment'].resample(freq).mean()\n",
    "\n",
    "daily_avg = resample_data(df, 'D')\n",
    "daily_avg.index = daily_avg.index.strftime('%Y-%m-%d')\n",
    "\n",
    "weekly_avg = resample_data(df, 'W')\n",
    "weekly_avg.index = weekly_avg.index.strftime('%Y-%m-%d')\n",
    "\n",
    "monthly_avg = resample_data(df, 'M')\n",
    "monthly_avg.index = monthly_avg.index.strftime('%Y-%m')\n",
    "\n",
    "yearly_avg = resample_data(df, 'Y')\n",
    "yearly_avg.index = yearly_avg.index.strftime('%Y')\n",
    "\n",
    "daily_avg.to_csv(project_title+'_daily_average.csv', index=True)\n",
    "weekly_avg.to_csv(project_title+'_weekly_average.csv', index=True)\n",
    "monthly_avg.to_csv(project_title+'_monthly_average.csv', index=True)\n",
    "yearly_avg.to_csv(project_title+'_yearly_average.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
